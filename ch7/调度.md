#### K8S的调度

##### 简介

`Scheduler`是kubernetes的调度器，主要的任务是把定义的pod分配到集群的节点上。需要考虑如下问题

* 公平：如何保证每个节点都能被分配资源
* 资源高效利用：集群所有资源最大化被使用
* 效率：调度的性能要好，能够尽快地对大批量的pod完成调度工作
* 灵活：允许用户根据自己的需要控制调度的逻辑

Scheduler是作为单纯的程序运行的，启动之后会一直监`API Server`，获取`podSpec.NodeName`为空的pod,对每个pod都会去创建一个binding,表示该pod应该放到哪个节点上



##### 调度过程 -2 steps-（预选和优选）

调度分为几个部分：首先是过滤掉不满足条件的节点<**预选**>，这个过程称为`predicate`;然后对通过的节点按照优先级排序，这个过程是`priority`<**优选**>；最后从中选择优先级最高的节点。如果中间任何一步骤有错误，就直接返回错误

**predicate**有一系列的算法开一使用：

* `PodFitsResources`：节点上剩余的资源是否大于pod请求的资源
* `PodFitsHost`：如果pod指定了NodeName，检查节点名称是否和NodeName匹配
* `PodFitsHostPorts`：节点上已经使用的port是否和pod申请的port冲突
* `PodSelectorMatches`：过滤掉 pod指定的label不匹配的节点
* `NoDiskConflict`:已经mount的volume和pod指定的volume不冲突，除非他们都是只读

**如果在predicate过程中没有合适的节点，pod会一直在pending状态，不断重试调度，直到有节点满足条件。经过这个步骤，如果有多个节点满足条件，就继续`priorities`过程：按照优先级大小对节点排序**

优先级由一系列键值对组成，键是该优先级项的名称，值是它的权重(该项的重要性)。这些优先级选项暴扣：

* `LeastRequestedPriority`: 通过计算CPU和Memeory使用率来决定权重，使用率越低权重越高，换句话说，这个优先级指标倾向于资源使用率更低的节点
* `BalanceResourceAllocation`:节点上cpu和memory使用率越接近，权重越高。这个要和`LeastRequestedPriority`一起使用
* `ImageLocalityPriority`：倾向于已经有使用镜像的节点，镜像总大小值越大，权重越高

通过算法对所有的优先级项目和权重进行计算，得出最终的结果

##### 自定义调度器

除了kubernetes自带的调度器，你也可以编写自己的调度器。通过`spec:schedulername`参数指定调度器的名字，可以为pod选择某个调度器进行调度。比如下面的pod选择`my-scheduler`进行调度，而不是默认的default-scheduler:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: annotation-second-scheduler
  labels:
    name: multischeduler-example
spec:
  schedulername: my-scheduler
  containers:
  - name: pod-witd-second-annotation-container
    image: gcr.io/google_containers/pause:2.0

```



##### 补充 volcano 的使用

```yaml
apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: test-job
spec:
  minAvailable: 3
  schedulerName: volcano
  policies:
    - event: PodEvicted
      action: RestartJob
  plugins:
    ssh: []
    env: []
    svc: []
  maxRetry: 5
  queue: default
  # Comment out the following section to enable volumes for job input/output.
  #volumes:
  #  - mountPath: "/myinput"
  #  - mountPath: "/myoutput"
  #    volumeClaimName: "testvolumeclaimname"
  #    volumeClaim:
  #      accessModes: [ "ReadWriteOnce" ]
  #      storageClassName: "my-storage-class"
  #      resources:
  #        requests:
  #          storage: 1Gi
  tasks:
    - replicas: 6
      name: "default-nginx"
      template:
        metadata:
          name: web
        spec:
          containers:
            - image: nginx
              imagePullPolicy: IfNotPresent
              name: nginx
              resources:
                requests:
                  cpu: "1"
          restartPolicy: OnFailure
```



```yaml
################################################
#                                              #
#    Demo for running TF tasks on Volcano      #
#                                              #
################################################
#
# This yaml used to demonstrate how to running a TF task via Volcano Job,
# the running sample program is from TF benchmark
# (https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks)
# The equivalent command when running locally:
#
#   python tf_cnn_benchmarks.py --num_gpus=1 --batch_size=32 --model=resnet50 --variable_update=parameter_server
#   --local_parameter_device=cpu --device=cpu --data_format=NHWC
#
# The output from ps or worker pod can be used to identify whether the TF cluster
# has been correctly configured:
#
#    (log from worker pod....)
#    2019-04-23 11:10:25.554248: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215]
#    Initialize GrpcChannelCache for job ps -> {0 -> tensorflow-benchmark-ps-0.tensorflow-benchmark:2222}
#    2019-04-23 11:10:25.554308: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215]
#    Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222}
#
#    (log from ps pod....)
#    2019-04-23 11:10:25.552827: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215]
#    Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}
#    2019-04-23 11:10:25.552861: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215]
#    Initialize GrpcChannelCache for job worker -> {0 -> tensorflow-benchmark-worker-0.tensorflow-benchmark:2222}
#
# **NOTES**: This example may take about an hour to finish. When running multiple jobs, please ensure enough resource
# is guaranteed for each of the worker pods.

apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: tensorflow-benchmark
spec:
  minAvailable: 3
  schedulerName: volcano
  plugins:
    env: []
    svc: []
  policies:
    - event: PodEvicted
      action: RestartJob
  tasks:
    - replicas: 1
      name: ps
      template:
        spec:
          imagePullSecrets:
            - name: default-secret
          containers:
            - command:
                - sh
                - -c
                - |
                  PS_HOST=`cat /etc/volcano/ps.host | sed 's/$/&:2222/g' | tr "\n" ","`;
                  WORKER_HOST=`cat /etc/volcano/worker.host | sed 's/$/&:2222/g' | tr "\n" ","`;
                  python tf_cnn_benchmarks.py --batch_size=32 --model=resnet50 --variable_update=parameter_server --flush_stdout=true --num_gpus=1 --local_parameter_device=cpu --device=cpu --data_format=NHWC --job_name=ps --task_index=${VK_TASK_INDEX} --ps_hosts=${PS_HOST} --worker_hosts=${WORKER_HOST}
              image: volcanosh/example-tf:0.0.1
              name: tensorflow
              ports:
                - containerPort: 2222
                  name: tfjob-port
              resources: {}
              workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks
          restartPolicy: OnFailure
    - replicas: 2
      name: worker
      policies:
        - event: TaskCompleted
          action: CompleteJob
      template:
        spec:
          imagePullSecrets:
            - name: default-secret
          containers:
            - command:
                - sh
                - -c
                - |
                  PS_HOST=`cat /etc/volcano/ps.host | sed 's/$/&:2222/g' | tr "\n" ","`;
                  WORKER_HOST=`cat /etc/volcano/worker.host | sed 's/$/&:2222/g' | tr "\n" ","`;
                  python tf_cnn_benchmarks.py --batch_size=32 --model=resnet50 --variable_update=parameter_server --flush_stdout=true --num_gpus=1 --local_parameter_device=cpu --device=cpu --data_format=NHWC --job_name=worker --task_index=${VK_TASK_INDEX} --ps_hosts=${PS_HOST} --worker_hosts=${WORKER_HOST}
              image: volcanosh/example-tf:0.0.1
              name: tensorflow
              ports:
                - containerPort: 2222
                  name: tfjob-port
              resources: {}
              workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks
          restartPolicy: OnFailure
```



















